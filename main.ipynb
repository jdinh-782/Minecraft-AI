{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "# import MalmoPython\n",
    "import malmo.MalmoPython as MalmoPython\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import errno\n",
    "from builtins import range\n",
    "from past.utils import old_div\n",
    "from collections import defaultdict, deque\n",
    "from timeit import default_timer as timer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Agent Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_space = [\"movenorth 1\", \"movesouth 1\", \"movewest 1\", \"moveeast 1\", \"jumpnorth 1\", \"movesprint 1\"]\n",
    "\n",
    "DIAMOND_WALL_Z = 25\n",
    "INITIAL_DISTANCE = DIAMOND_WALL_Z - (-3)\n",
    "state_actions =  {\n",
    "            'diamond_block': 100,\n",
    "            'packed_ice': 1,\n",
    "            'log': -10,\n",
    "            'water': -5 \n",
    "            }\n",
    "class Racer(object):\n",
    "    def __init__(self, alpha=0.3, gamma=1, n=1):\n",
    "        \"\"\"Constructing an RL agent.\n",
    "\n",
    "        Args\n",
    "            alpha:  <float>  learning rate      (default = 0.3)\n",
    "            gamma:  <float>  value decay rate   (default = 1)\n",
    "            n:      <int>    number of back steps to update (default = 1)\n",
    "        \"\"\"\n",
    "        self.epsilon = 0.2  # chance of taking a random action instead of the best\n",
    "        self.q_table = {}\n",
    "        self.n, self.alpha, self.gamma = n, alpha, gamma\n",
    "        self.actions_taken = defaultdict(lambda: 0, {})\n",
    "        self.num_actions = 0\n",
    "        self.currentState = {\n",
    "            'agent_x': 1,\n",
    "            'agent_z': -4,\n",
    "            'goal_z': 25,\n",
    "            'has_water_left': False,\n",
    "            'has_water_right': False,\n",
    "            'has_water_forward': False,\n",
    "            'has_wood_left': False,\n",
    "            'has_wood_right': False,\n",
    "            'has_wood_forward': False,\n",
    "        }\n",
    "\n",
    "    def clear_actions(self):\n",
    "        \"\"\"Resets the actions in case of a new iteration to fetch. \"\"\"\n",
    "        self.actions_taken = defaultdict(lambda: 0, {})\n",
    "        self.num_actions = 0\n",
    "        self.currentState = {\n",
    "            'agent_x': 1,\n",
    "            'agent_z': -4,\n",
    "            'goal_z': 25,\n",
    "            'has_water_left': False,\n",
    "            'has_water_right': False,\n",
    "            'has_water_forward': False,\n",
    "            'has_wood_left': False,\n",
    "            'has_wood_right': False,\n",
    "            'has_wood_forward': False,\n",
    "        }\n",
    "        #resetDefault\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_curr_location():\n",
    "        # get the world state\n",
    "        world_state = agent_host.peekWorldState()\n",
    "        location = tuple()\n",
    "        initial_location = (1, 2, -3)\n",
    "#         print(\"get curr location\")\n",
    "        if world_state.number_of_observations_since_last_state > 0:\n",
    "            msg = world_state.observations[-1].text\n",
    "#             print(msg)\n",
    "            observations = json.loads(msg)\n",
    "#             print(observations)\n",
    "\n",
    "            # get curr location from json\n",
    "            xpos = observations.get(u'XPos',0)\n",
    "            ypos = observations.get(u'YPos',0)\n",
    "            zpos = observations.get(u'ZPos',0)\n",
    "            location = (xpos, ypos, zpos)\n",
    "            print(\"LOCATION: \", location)\n",
    "            return location\n",
    "        else:\n",
    "            return initial_location\n",
    "    \n",
    "    \n",
    "        \n",
    "    def calculate_reward(self):\n",
    "        agent_z = Racer.get_curr_location()[2]\n",
    "        dist_from_wall = DIAMOND_WALL_Z - agent_z\n",
    "        dist_traveled = INITIAL_DISTANCE - dist_from_wall\n",
    "        reward = math.floor((dist_traveled*100)/INITIAL_DISTANCE)\n",
    "        return reward\n",
    "        \n",
    "    def present_actions(self, agent_host):\n",
    "        \"\"\"Calculates the reward points for the current inventory.\n",
    "\n",
    "        Args\n",
    "            agent_host: the host object\n",
    "\n",
    "        Returns\n",
    "            reward:     <float> current reward from world state\n",
    "        \"\"\"\n",
    "        current_r = 0\n",
    "        #time.sleep(0.1)\n",
    "        \n",
    "        # calculate agent's current location\n",
    "        # measure the distance between current location and the diamond wall\n",
    "        \n",
    "\n",
    "#         for item, counts in self.actions_taken.items():\n",
    "#             current_r += rewards_map[item] * counts\n",
    "#             current_r += rewards_map[item]\n",
    "        current_r = self.calculate_reward()\n",
    "\n",
    "        #agent_host.sendCommand('quit')\n",
    "        #time.sleep(0.25)\n",
    "        return current_r\n",
    "    \n",
    "    def is_solution(reward):\n",
    "        \"\"\"If the reward equals to the maximum reward possible returns True, False otherwise. \"\"\"\n",
    "        return reward == 100\n",
    "        # THIS VALUE IS SUBJECT TO CHANGE\n",
    "    \n",
    "    # do we need the below function?\n",
    "    def get_possible_actions(self, agent_host, is_first_action=False):\n",
    "        \"\"\"Returns all possible actions that can be done at the current state. \"\"\"\n",
    "#         print(\"in get possible actions\")\n",
    "        action_list = []\n",
    "#         if not is_first_action:\n",
    "#             # Not allowing Odie to come back empty.\n",
    "# #             action_list = ['present_gift']\n",
    "#             action_list = ['present_actions']\n",
    "        action_list.extend(actions_space)\n",
    "        return action_list\n",
    "\n",
    "    def get_curr_state(self): #NEEDS WORK\n",
    "        \"\"\"Creates a unique identifier for a state.\n",
    "        Probably want to see the 3 blocks in front of it as well. Like if there is an obstacle. Check Z+1\n",
    "        \"\"\"\n",
    "        state = self.get_curr_location()\n",
    "        return state\n",
    "    \n",
    "    def choose_action(self, curr_state, possible_actions, eps):\n",
    "        \"\"\"Chooses an action according to eps-greedy policy. \"\"\"\n",
    "#         print(\"in choose action\")\n",
    "        if curr_state not in self.q_table:\n",
    "            self.q_table[curr_state] = {}\n",
    "        for action in possible_actions:\n",
    "            if action not in self.q_table[curr_state]:\n",
    "                self.q_table[curr_state][action] = 0\n",
    "        \n",
    "        rnd = random.random()\n",
    "        if rnd <= eps:\n",
    "            a = random.randint(0, len(possible_actions) - 1)\n",
    "            return possible_actions[a]\n",
    "        else:\n",
    "            # copy dict{actions: q-values} of q_table[curr_state]\n",
    "            state_actions =self.q_table[curr_state]\n",
    "            # find the max q-value\n",
    "            max_q = max(state_actions.values())\n",
    "            # find the list of actions that return the maximum q-value\n",
    "            max_actions = [action for action, value in state_actions.items() if value == max_q]\n",
    "            # pick a random action from the max_actions list\n",
    "            max_rand = random.randint(0, len(max_actions) - 1)\n",
    "            return max_actions[max_rand]\n",
    "                     \n",
    "    def act(self, agent_host, action):\n",
    "        print(action + \",\", end = \" \")\n",
    "        agent_host.sendCommand(action)\n",
    "        return 0\n",
    "\n",
    "    def update_q_table(self, tau, S, A, R, T): # THIS OR THE COPY OF IT NEEDS WORK!\n",
    "        \"\"\"Performs relevant updates for state tau.\n",
    "\n",
    "        Args\n",
    "            tau: <int>  state index to update\n",
    "            S:   <dequqe>   states queue\n",
    "            A:   <dequqe>   actions queue\n",
    "            R:   <dequqe>   rewards queue\n",
    "            T:   <int>      terminating state index\n",
    "            \n",
    "        \"\"\"\n",
    "        curr_s, curr_a, curr_r = S.popleft(), A.popleft(), R.popleft()\n",
    "        G = sum([self.gamma ** i * R[i] for i in range(len(S))])\n",
    "        if tau + self.n < T:\n",
    "            G += self.gamma ** self.n * self.q_table[S[-1]][A[-1]]\n",
    "\n",
    "        old_q = self.q_table[curr_s][curr_a]\n",
    "        self.q_table[curr_s][curr_a] = old_q + self.alpha * (G - old_q)\n",
    "        \n",
    "        ###Incomplete version of Q Table. Figuring out best way to handle. TD seems to be a better algo.\n",
    "    def update_q_table2(self, S, A, R, T):\n",
    "        \"\"\"Performs relevant updates for the Q-values.\n",
    "        TD(0) implementation of SARSA algorithm. Better for this use case.\n",
    "        Args\n",
    "            S:   <dequqe>   states queue\n",
    "            A:   <dequqe>   actions queue\n",
    "            R:   <dequqe>   rewards queue\n",
    "            T:   <int>      terminating state index\n",
    "\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "        for i in range(len(S)-1, -1, -1):\n",
    "            G = self.gamma * G + R[i+1]\n",
    "            old_q = self.q_table[S[i]][A[i]]\n",
    "            self.q_table[S[i]][A[i]] = old_q + self.alpha * (G - old_q)\n",
    "\n",
    "    def best_policy(self, agent_host):\n",
    "        \"\"\"Reconstructs the best action list according to the greedy policy. \"\"\"\n",
    "        self.clear_actions()\n",
    "        policy = []\n",
    "        current_r = 0\n",
    "        is_first_action = True\n",
    "        next_a = \"\"\n",
    "        while next_a != \"present_actions\":\n",
    "            curr_state = self.get_curr_state()\n",
    "            possible_actions = self.get_possible_actions(agent_host, is_first_action)\n",
    "            next_a = self.choose_action(curr_state, possible_actions, 0)\n",
    "            policy.append(next_a)\n",
    "            is_first_action = False\n",
    "            current_r = self.act(agent_host, next_a)\n",
    "        print(' with reward %.1f' % (current_r))\n",
    "        return is_solution(current_r)\n",
    "        #print 'Best policy so far is %s with reward %.1f' % (policy, current_r)\n",
    "        \n",
    "    def run(self, agent_host): ###THIS NEEDS WORK ALONG W THE Q TABLE!\n",
    "        \"\"\"Learns the process to reach the diamonds\"\"\"\n",
    "        S, A, R = deque(), deque(), deque()\n",
    "        present_reward = 0\n",
    "        done_update = False\n",
    "        while not done_update:\n",
    "            s0 = self.get_curr_state()\n",
    "            possible_actions = self.get_possible_actions(agent_host, True)\n",
    "            a0 = self.choose_action(s0, possible_actions, self.epsilon)\n",
    "            S.append(s0)\n",
    "            A.append(a0)\n",
    "            R.append(0)\n",
    "\n",
    "            T = sys.maxsize\n",
    "            for t in range(sys.maxsize):\n",
    "                time.sleep(0.1)\n",
    "                if t < T:\n",
    "                    print(\"action: \", A[-1])\n",
    "                    \n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    current_r = self.act(agent_host, A[-1])\n",
    "                    R.append(current_r)\n",
    "\n",
    "                    if A[-1] == \"present_actions\": #We do not needs this.\n",
    "                        # Terminating state\n",
    "                        T = t + 1\n",
    "                        S.append('Term State')\n",
    "                        present_reward = current_r\n",
    "                        print(\"Reward:\", present_reward)\n",
    "                    else:\n",
    "                        s = self.get_curr_state()\n",
    "                        S.append(s)\n",
    "                        possible_actions = self.get_possible_actions(agent_host)\n",
    "                        next_a = self.choose_action(s, possible_actions, self.epsilon)\n",
    "                        A.append(next_a)\n",
    "\n",
    "                tau = t - self.n + 1\n",
    "                if tau >= 0:\n",
    "                    self.update_q_table(tau, S, A, R, T)\n",
    "\n",
    "                if tau == T - 1:\n",
    "                    while len(S) > 1:\n",
    "                        tau = tau + 1\n",
    "                        self.update_q_table(tau, S, A, R, T)\n",
    "                    done_update = True\n",
    "                    break\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Racer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_map =  {'diamond_block': 100, 'packed_ice': 1, 'log': -10, 'water': -5}\n",
    "actions_space = [\"move 1\", \"move -1\", \"strafe 1\", \"strafe -1\"]  \n",
    "# REMINDER: VALUE OF EACH ACTION IS THE SPEED, NOT NUMBER OF TIMES\n",
    "# move  1    full speed ahead\n",
    "# move -1    full speed backwards\n",
    "# strafe 1   moves right at full speed\n",
    "# strafe -1  moves left at full speed\n",
    "# turn 1     turns full speed right\n",
    "# turn -1    turns full speed left\n",
    "# jump 1/0   starts/stops jumping\n",
    "\n",
    "\n",
    "\n",
    "# helper function to update the actions of the current state\n",
    "def get_curr_state_helper(items):\n",
    "    curr_state_items = tuple()\n",
    "    for key, val in items:\n",
    "        curr_state_items += (key, val)\n",
    "    \n",
    "    return curr_state_items\n",
    "\n",
    "\n",
    "# helper function to choose an action based on the current state and possible actions\n",
    "def choose_action_helper(curr_state, possible_actions, eps, q_table):\n",
    "    rnd = random.random()\n",
    "    \n",
    "    action_chosed = \"\"\n",
    "    if rnd < eps:\n",
    "#         print(\"random choice\")\n",
    "        a = random.randint(0, len(possible_actions) - 1)\n",
    "        action_chosed = possible_actions[a]\n",
    "    else:\n",
    "#         print(\"greedy choice\")\n",
    "        d = q_table[curr_state]  # create dict of q_table with (action, qValue) pairs\n",
    "        max_qValue = max(d.values())  # extract maximum Q-value\n",
    "        \n",
    "        ls = [k for k, v in d.items() if v == max_qValue]  # get all actions with same maximum Q-value\n",
    "#         print(\"ls: \", ls)\n",
    "\n",
    "        # randomly choose from list of possible actions with same maximum Q-value\n",
    "        a = random.randint(0, len(ls) - 1)  \n",
    "        action_chosed = ls[a]\n",
    "    \n",
    "#     print(\"action chosed: \", action_chosed)\n",
    "    \n",
    "    return action_chosed\n",
    "            \n",
    "\n",
    "class Racer(object):\n",
    "    # initializing the parameters for the agent\n",
    "    def __init__(self, alpha=0.3, gamma=1, n=1):\n",
    "        \"\"\"Constructing an RL agent.\n",
    "\n",
    "        Args\n",
    "            alpha:  <float>  learning rate      (default = 0.3)\n",
    "            gamma:  <float>  value decay rate   (default = 1)\n",
    "            n:      <int>    number of back steps to update (default = 1)\n",
    "        \"\"\"\n",
    "        self.epsilon = 0.4  # chance of taking a random action instead of the best, default 0.2\n",
    "        self.q_table = {}   # track agent decisions and rewards\n",
    "        self.n, self.alpha, self.gamma = n, alpha, gamma\n",
    "        self.actions_taken = defaultdict(lambda: 0, {})\n",
    "        self.num_actions = 0\n",
    "        \n",
    "    def clear_actions(self):\n",
    "        \"\"\"Resets the actions taken in case of a new attempt to fetch.\"\"\"\n",
    "        self.actions_taken = defaultdict(lambda: 0, {})\n",
    "        self.num_actions = 0\n",
    "    \n",
    "    # where the agent will present the final list of actions taken \n",
    "    def present_result(self, agent_host):\n",
    "        \"\"\"Calculates the reward points for the current list of actions taken.\n",
    "\n",
    "        Args\n",
    "            agent_host: the host object\n",
    "\n",
    "        Returns\n",
    "            reward:     <float> current reward from world state\n",
    "        \"\"\"\n",
    "        current_r = 0\n",
    "        # time.sleep(0.1)\n",
    "\n",
    "        for item, counts in self.actions_taken.items():\n",
    "            current_r += rewards_map[item] * counts\n",
    "\n",
    "        agent_host.sendCommand('quit')\n",
    "        #time.sleep(0.25)\n",
    "        return current_r\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_solution(reward):\n",
    "        \"\"\"If the reward equals to the maximum reward possible returns True, False otherwise. \"\"\"\n",
    "        return reward == 0  # default 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_obj_locations(agent_host):\n",
    "        \"\"\"Queries for the object's location in the world.\n",
    "\n",
    "        As a side effect it also returns the Racer's location.\n",
    "        \"\"\"\n",
    "        objs = {}\n",
    "        while True:\n",
    "            world_state = agent_host.getWorldState()\n",
    "            if world_state.number_of_observations_since_last_state > 0:\n",
    "                # print(world_state.observations)\n",
    "                msg = world_state.observations[-1].text\n",
    "                data = json.loads(msg)\n",
    "                # print(\"data:\", data)\n",
    "                # print(\"data.keys():\", data.keys())\n",
    "\n",
    "                for obj in data['floorAll']:\n",
    "                    # print(obj, '\\n')\n",
    "                    \n",
    "                    # need to work on finding specific coordinates for each object\n",
    "                    # prob can just extract coordinates of each entity from xml\n",
    "                    if obj in objs:\n",
    "                        objs[obj] += 1\n",
    "                    else:\n",
    "                        objs[obj] = 0\n",
    "                \n",
    "                objs['Racer'] = (data['Yaw'], data['XPos'], data['ZPos'])\n",
    "                print(\"objs:\", objs)\n",
    "                return objs\n",
    "    \n",
    "    \n",
    "    # heavy emphasis on this function\n",
    "    # WORK ON THIS\n",
    "    def get_possible_actions(self, agent_host, is_first_action=False):\n",
    "        \"\"\"Returns all possible actions that can be done at the current state. \"\"\"\n",
    "        action_list = []\n",
    "        if not is_first_action:\n",
    "            # Not allowing the Racer to come back empty.\n",
    "            action_list = ['present_result']\n",
    "            \n",
    "        nearby_obj = self.get_obj_locations(agent_host)\n",
    "        # print(\"nearby_obj\", nearby_obj)\n",
    "        if len(nearby_obj) > 1:\n",
    "            action_list.extend([item for item in nearby_obj.keys() if item != 'Racer'])\n",
    "            \n",
    "        return action_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_curr_state(self):\n",
    "        \"\"\"Creates a unique identifier for a state.\n",
    "\n",
    "        The state is defined as the items in the agent's list of choices. Notice that the state has to be sorted -- otherwise\n",
    "        differnt order in the list will be different states.\n",
    "        \"\"\"\n",
    "        return get_curr_state_helper(self.actions_taken.items())\n",
    "    \n",
    "    def choose_action(self, curr_state, possible_actions, eps):\n",
    "        \"\"\"Chooses an action according to eps-greedy policy. \"\"\"\n",
    "        if curr_state not in self.q_table:\n",
    "            self.q_table[curr_state] = {}\n",
    "        for action in possible_actions:\n",
    "            if action not in self.q_table[curr_state]:\n",
    "                self.q_table[curr_state][action] = 0\n",
    "        \n",
    "        return choose_action_helper(curr_state, possible_actions, eps, self.q_table)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # heavy emphasis on this function\n",
    "    # completes the action to either move in a certain direction or jump \n",
    "    # WORK ON THIS\n",
    "    def complete_action(self, agent_host, action):\n",
    "        print(\"action completed:\", action)\n",
    "        agent_host.sendCommand(action)\n",
    "        \n",
    "        self.actions_taken[action] += 1\n",
    "        self.num_actions += 1\n",
    "        time.sleep(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # act function determines if agent should present its results or do another action\n",
    "    def act(self, agent_host, action):\n",
    "        print(action + \",\", end = \" \")\n",
    "        if action == 'present_result':\n",
    "            return self.present_result(agent_host)\n",
    "        else:\n",
    "            self.complete_action(agent_host, action)\n",
    "        return 0\n",
    "    \n",
    "    def update_q_table(self, tau, S, A, R, T):\n",
    "        \"\"\"Performs relevant updates for state tau.\n",
    "        Args\n",
    "            tau: <int>  state index to update\n",
    "            S:   <dequqe>   states queue\n",
    "            A:   <dequqe>   actions queue\n",
    "            R:   <dequqe>   rewards queue\n",
    "            T:   <int>      terminating state index\n",
    "        \"\"\"\n",
    "        curr_s, curr_a, curr_r = S.popleft(), A.popleft(), R.popleft()\n",
    "        G = sum([self.gamma ** i * R[i] for i in range(len(S))])\n",
    "        if tau + self.n < T:\n",
    "            G += self.gamma ** self.n * self.q_table[S[-1]][A[-1]]\n",
    "\n",
    "        old_q = self.q_table[curr_s][curr_a]\n",
    "        self.q_table[curr_s][curr_a] = old_q + self.alpha * (G - old_q)\n",
    "    \n",
    "    def best_policy(self, agent_host):\n",
    "        \"\"\"Reconstructs the best action list according to the greedy policy.\"\"\"\n",
    "        self.clear_actions()\n",
    "        policy = []\n",
    "        current_r = 0\n",
    "        is_first_action = True\n",
    "        next_a = \"\"\n",
    "        \n",
    "        while next_a != \"present_result\":\n",
    "            curr_state = self.get_curr_state()\n",
    "            possible_actions = self.get_possible_actions(agent_host, is_first_action)\n",
    "            next_a = self.choose_action(curr_state, possible_actions, 0)\n",
    "            policy.append(next_a)\n",
    "            is_first_action = False\n",
    "            current_r = self.act(agent_host, next_a)\n",
    "        print(' with reward %.1f' % (current_r))\n",
    "        #print 'Best policy so far is %s with reward %.1f' % (policy, current_r)\n",
    "        \n",
    "        return self.is_solution(current_r)\n",
    "    \n",
    "    def run(self, agent_host):\n",
    "        \"\"\"Learns the process to compile the best result for reaching the goal.\"\"\"     \n",
    "        S, A, R = deque(), deque(), deque()\n",
    "        present_reward = 0\n",
    "        done_update = False\n",
    "        \n",
    "        while not done_update:\n",
    "            s0 = self.get_curr_state()\n",
    "            possible_actions = self.get_possible_actions(agent_host, True)\n",
    "            a0 = self.choose_action(s0, possible_actions, self.epsilon)\n",
    "            S.append(s0)\n",
    "            A.append(a0)\n",
    "            R.append(0)\n",
    "            \n",
    "            T = sys.maxsize\n",
    "            for t in range(sys.maxsize):\n",
    "                time.sleep(0.1)\n",
    "                if t < T:\n",
    "                    current_r = self.act(agent_host, A[-1])\n",
    "                    R.append(current_r)\n",
    "                    \n",
    "                    if A[-1] == \"present_result\":\n",
    "                        # Terminating state\n",
    "                        T = t + 1\n",
    "                        S.append('Term State')\n",
    "                        present_reward = current_r\n",
    "                        print(\"Reward:\", present_reward)\n",
    "                    else:\n",
    "                        s = self.get_curr_state()\n",
    "                        S.append(s)\n",
    "                        possible_actions = self.get_possible_actions(agent_host)\n",
    "                        next_a = self.choose_action(s, possible_actions, self.epsilon)\n",
    "                        A.append(next_a)\n",
    "                        \n",
    "                    tau = t - self.n + 1\n",
    "                    if tau >= 0:\n",
    "                        self.update_q_table(tau, S, A, R, T)\n",
    "\n",
    "                    if tau == T - 1:\n",
    "                        while len(S) > 1:\n",
    "                            tau = tau + 1\n",
    "                            self.update_q_table(tau, S, A, R, T)\n",
    "                        done_update = True\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Caught std::exception: unrecognised option '-f'\n",
      "\n",
      "Malmo version: 0.36.0\n",
      "\n",
      "Allowed options:\n",
      "  -h [ --help ]         show description of allowed options\n",
      "  --test                run this as an integration test\n",
      "\n",
      "\n",
      "Loading mission from xmls/testmap.txt\n",
      "n = 10\n",
      "\n",
      "Waiting for the mission to start on trial 0\n",
      "Mission running...\n",
      "1 Learning Q-Table: objs: {'log': 75, 'grass': 258, 'packed_ice': 105, 'Racer': (0.0, 0.0, -4.0)}\n",
      "log, action completed: log\n",
      "objs: {'log': 75, 'grass': 258, 'packed_ice': 105, 'Racer': (0.0, 0.0, -4.0)}\n",
      "present_result, Reward: -10\n"
     ]
    }
   ],
   "source": [
    "if sys.version_info[0] == 2:\n",
    "    sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)  # flush print output immediately\n",
    "else:\n",
    "    import functools\n",
    "    print = functools.partial(print, flush=True)\n",
    "    \n",
    "\n",
    "expected_reward = 3390\n",
    "my_client_pool = MalmoPython.ClientPool()\n",
    "my_client_pool.add(MalmoPython.ClientInfo(\"127.0.0.1\", 10000))\n",
    "\n",
    "\n",
    "# Create default Malmo objects:\n",
    "agent_host = MalmoPython.AgentHost()\n",
    "try:\n",
    "    agent_host.parse( sys.argv )\n",
    "except RuntimeError as e:\n",
    "    print('ERROR:',e)\n",
    "    print(agent_host.getUsage())\n",
    "    exit(1)\n",
    "#     sys.exit()\n",
    "if agent_host.receivedArgument(\"help\"):\n",
    "    print(agent_host.getUsage())\n",
    "    exit(0)\n",
    "#     sys.exit()\n",
    "\n",
    "\n",
    "# load in world map\n",
    "world_num = random.randint(0, 24)\n",
    "# mission_file = 'xmls/world_{world_num}.txt'.format(world_num = world_num)\n",
    "mission_file = 'xmls/testmap.txt'  # test map \n",
    "with open(mission_file, 'r') as f:\n",
    "    print(\"Loading mission from %s\" % mission_file)\n",
    "    missionXML = f.read()\n",
    "\n",
    "    \n",
    "# initiate the Racer object\n",
    "num_reps = 1\n",
    "n = 10\n",
    "racer = Racer(n=n)\n",
    "print(\"n =\", n)\n",
    "racer.clear_actions()\n",
    "\n",
    "\n",
    "for iRepeat in range(num_reps):\n",
    "    my_mission = MalmoPython.MissionSpec(missionXML, True)\n",
    "    my_mission_record = MalmoPython.MissionRecordSpec()  # Records nothing by default\n",
    "    my_mission.requestVideo(1260, 960)\n",
    "    my_mission.setViewpoint(0)\n",
    "\n",
    "    # Attempt to start a mission:\n",
    "    max_retries = 3\n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            agent_host.startMission( my_mission, my_client_pool, my_mission_record, 0, \"Racer\" )\n",
    "            break\n",
    "        except RuntimeError as e:\n",
    "            if retry == max_retries - 1:\n",
    "                print(\"Error starting mission:\",e)\n",
    "                exit(1)\n",
    "                # sys.exit()\n",
    "            else:\n",
    "                time.sleep(2)\n",
    "\n",
    "    # Loop until mission starts:\n",
    "    print(\"\\nWaiting for the mission to start on trial\", iRepeat)\n",
    "    world_state = agent_host.getWorldState()\n",
    "    \n",
    "    while not world_state.has_mission_begun:\n",
    "        time.sleep(0.1)\n",
    "        world_state = agent_host.getWorldState()\n",
    "        for error in world_state.errors:\n",
    "            print(\"Error:\",error.text)\n",
    "    \n",
    "    # Loop until mission ends:\n",
    "    print(\"Mission running...\")\n",
    "#     while world_state.is_mission_running:\n",
    "#         time.sleep(0.1)\n",
    "#         world_state = agent_host.getWorldState()\n",
    "\n",
    "#         for error in world_state.errors:\n",
    "#             print(\"Error:\",error.text)\n",
    "            \n",
    "    # Check for actions and rewards here\n",
    "    if (iRepeat + 1) % 5 == 0:\n",
    "            print((iRepeat+1), 'Showing best policy:', end = \" \")\n",
    "            found_solution = racer.best_policy(agent_host)\n",
    "            if found_solution:\n",
    "                print('Found solution')\n",
    "                print('Done')\n",
    "                break\n",
    "    else:\n",
    "        print((iRepeat+1), 'Learning Q-Table:', end = \" \")\n",
    "        racer.run(agent_host)\n",
    "    \n",
    "    racer.clear_actions()  # clear list of actions for next run\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "print(\"\\n\\nMission ended.\")\n",
    "# Mission has ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
