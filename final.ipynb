{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import MalmoPython\n",
    "import malmo.MalmoPython as MalmoPython\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import errno\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "from timeit import default_timer as timer\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_q_table(q_table, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(q_table, f)\n",
    "\n",
    "def load_q_table(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        q_table = json.load(f)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed()\n",
    "world_num = random.randint(0, 25)\n",
    "world_map = open('xmls/world_{world_num}.txt'.format(world_num = world_num), 'r')\n",
    "lines = world_map.readlines()\n",
    "logs = [] #tuple (block, (x,y,z))\n",
    "water = []\n",
    "diamond = []\n",
    "obs_set = set()\n",
    "\n",
    "print(\"world_map {}\".format(world_num))\n",
    "def addObjects(line, block_name, obs_list):\n",
    "    terms = line.split()\n",
    "    coords = []\n",
    "\n",
    "    for i in range(1,7):\n",
    "        start = terms[i].index('\"') + 1\n",
    "        end = terms[i].index('\"', start)\n",
    "        coords.append(int(terms[i][start:end]))\n",
    "\n",
    "\n",
    "    if coords[0] < -4: #IF X is OUTSIDE OF OUR DESIRED RANGE -4, 4 we just dip.\n",
    "        return\n",
    "    if coords[2] > 30: #IF Z is outside. dip.\n",
    "        return\n",
    "    \n",
    "    if coords[0] == coords[3]:\n",
    "        x_range = [coords[0]]\n",
    "    else:\n",
    "        x_range = [x for x in range(min(coords[0],coords[3]), max(coords[0],coords[3])+ 1)] #do we need + 1 to make range inclusive?\n",
    "    \n",
    "    if coords[1] == coords[4]:\n",
    "        y_range = [coords[1]]\n",
    "    elif block_name == \"diamond_block\":\n",
    "        y_range = [1,2] # WE DON'T NEED TO ADD DIAMOND_BLOCKS THAT HIGHER THAN 6 IN Y-AXIS\n",
    "    elif coords[4] != coords[1]: #this was initially 4 1 swapped. not sure why?\n",
    "        y_range = [y for y in range(min(coords[1],coords[4]), max(coords[1],coords[4]))]\n",
    "    \n",
    "    if coords[2] == coords[5]:\n",
    "        z_range = [coords[2]]\n",
    "    else:\n",
    "        z_range = [z for z in range(coords[2], coords[5])] #this generally only happens for walls.\n",
    "    print(coords)\n",
    "\n",
    "    #this spawns way too many duplicates.\n",
    "    for x in x_range:\n",
    "        for y in y_range:\n",
    "            for z in z_range:\n",
    "                obs_list.append((block_name, (x, y, z)))\n",
    "for line in lines:\n",
    "    if \"DrawCuboid\" in line:\n",
    "        if \"log\" in line:\n",
    "            addObjects(line, \"log\", logs)\n",
    "        if \"diamond_block\" in line:\n",
    "            addObjects(line, \"diamond_block\", diamond)\n",
    "    if \"DrawLine\" in line:\n",
    "        addObjects(line, \"water\", water)\n",
    "# print(logs)\n",
    "# print(diamond)\n",
    "# print(water)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ContinuousMovementCommands\n",
    "actions_space = ['move 1', 'strafe 1', 'strafe -1', 'jump 1']\n",
    "# REMINDER: VALUE OF EACH ACTION IS THE SPEED, NOT NUMBER OF TIMES\n",
    "# move  1    full speed ahead\n",
    "# move -1    full speed backwards\n",
    "# strafe 1   moves right at full speed\n",
    "# strafe -1  moves left at full speed\n",
    "# turn 1     turns full speed right\n",
    "# turn -1    turns full speed left\n",
    "# jump 1/0   starts/stops jumping\n",
    "\n",
    "\n",
    "INITIAL_LOCATION = (1, 2, 0)\n",
    "DIAMOND_WALL_Z = 30\n",
    "INITIAL_DISTANCE = DIAMOND_WALL_Z\n",
    "reward_map =  {\n",
    "            'diamond_block': 100,\n",
    "            'packed_ice': 1,\n",
    "            'log': -20,\n",
    "            'water': -2\n",
    "            }\n",
    "\n",
    "importantObjects = logs + diamond + water\n",
    "\n",
    "class Racer(object):\n",
    "    def __init__(self, alpha=0.3, gamma=1, epsilon=0.3):\n",
    "        \"\"\"Constructing an RL agent.\n",
    "\n",
    "        Args\n",
    "            alpha:  <float>  learning rate      (default = 0.3)\n",
    "            gamma:  <float>  value decay rate   (default = 1)\n",
    "            n:      <int>    number of back steps to update (default = 10)\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon  # chance of taking a random action instead of the best\n",
    "        self.q_table = {}\n",
    "        self.alpha, self.gamma = alpha, gamma\n",
    "        self.actions_taken = []\n",
    "        self.num_actions = 0\n",
    "        self.reward = 0\n",
    "        self.diamond_reached = False\n",
    "        self.obstacles_hit = False\n",
    "        self.obstacles_avoided = 0\n",
    "        self.timer = time.time()\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if False: # True if you want to see more information\n",
    "            self.logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            self.logger.setLevel(logging.INFO)\n",
    "        self.logger.handlers = []\n",
    "        self.logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "        \n",
    "    def clear_actions(self):\n",
    "        \"\"\"Resets the actions in case of a new iteration to fetch. \"\"\"\n",
    "        self.actions_taken = []\n",
    "        self.num_actions = 0\n",
    "        self.reward = 0\n",
    "        self.diamond_reached = False\n",
    "        self.obstacles_hit = False\n",
    "        \n",
    "    def get_possible_actions(self, agent_host, term_flag):\n",
    "        \"\"\"Returns all possible actions that can be done at the current state. \"\"\"\n",
    "#         print(\"in get possible actions\")\n",
    "        action_list = []\n",
    "        if not term_flag:\n",
    "            action_list.extend(actions_space)\n",
    "        return action_list\n",
    "    \n",
    "    def choose_action(self, curr_state, possible_actions, eps):\n",
    "        \"\"\"Chooses an action according to eps-greedy policy. \"\"\"\n",
    "        if curr_state not in self.q_table:\n",
    "            self.q_table[curr_state] = {}\n",
    "        for action in possible_actions:\n",
    "            if action not in self.q_table[curr_state]:\n",
    "                self.q_table[curr_state][action] = 0\n",
    "        \n",
    "        rnd = random.random()\n",
    "        if rnd <= eps:\n",
    "            a = random.randint(0, len(possible_actions) - 1)\n",
    "            return possible_actions[a]\n",
    "        else:\n",
    "            # copy dict{actions: q-values} of q_table[curr_state]\n",
    "            state_actions = self.q_table[curr_state]\n",
    "            # find the max q-value\n",
    "            max_q = max(state_actions.values())\n",
    "            # find the list of actions that return the maximum q-value\n",
    "            max_actions = [action for action, value in state_actions.items() if value == max_q]\n",
    "            # pick a random action from the max_actions list\n",
    "            max_rand = random.randint(0, len(max_actions) - 1)\n",
    "            return max_actions[max_rand]\n",
    "        \n",
    "    def get_curr_location(self, agent_host):\n",
    "        # get the world state\n",
    "        world_state = agent_host.peekWorldState()\n",
    "        location = tuple()\n",
    "        if world_state.number_of_observations_since_last_state > 0:\n",
    "            msg = world_state.observations[-1].text\n",
    "            observations = json.loads(msg)\n",
    "            # get curr location from json\n",
    "            xpos = observations.get(u'XPos',0)\n",
    "            ypos = observations.get(u'YPos',0)\n",
    "            zpos = observations.get(u'ZPos',0)\n",
    "            location = (xpos, ypos, zpos)\n",
    "#             print(\"LOCATION: \", location)\n",
    "            return location\n",
    "        else:\n",
    "            return INITIAL_LOCATION\n",
    "        \n",
    "    def get_obj_locations(self, agent_host):\n",
    "        return logs + water + diamond\n",
    "    \n",
    "    def calculate_dist_reward(self, agent_host, currentState, timer):\n",
    "        #agent_z = self.get_curr_location(agent_host)[2]\n",
    "        agent_z = currentState[2]\n",
    "        dist_from_wall = DIAMOND_WALL_Z -agent_z\n",
    "        dist_traveled = INITIAL_DISTANCE - dist_from_wall\n",
    "#         print(\"traveled: \", dist_traveled)\n",
    "#         print(\"Dist From Wall: \", dist_from_wall)\n",
    "        reward = math.floor((dist_traveled*100)/INITIAL_DISTANCE)\n",
    "        if self.diamond_reached == True:\n",
    "            time_elapsed = time.time() - timer #find current runtime, minus time of starting.\n",
    "            reward += 200 // time_elapsed\n",
    "#             print(\"bonus reward: \", reward)\n",
    "        return reward\n",
    "    \n",
    "    def eval_current_state(self, agent_host, current_state, timer):\n",
    "#         print(\"current_state passed in eval:\", current_state)\n",
    "        #maybe need to set obstacles hit and diamond to false? but since its in a class shud be ok.\n",
    "        agent_loc = self.get_curr_location(agent_host)\n",
    "#         print(\"agent_loc =\", agent_loc)\n",
    "        nearby_objects = self.get_obj_locations(agent_host)\n",
    "        reward = self.calculate_dist_reward(agent_host, current_state, timer) \n",
    "        ######################################## MOVING THIS DOWN TO ACCOUNT FOR TIMER!!\n",
    "        for obj in nearby_objects:\n",
    "            obj_type, obj_loc = obj\n",
    "            obj_x, obj_y, obj_z = obj_loc\n",
    "\n",
    "            # Check for logs\n",
    "            if obj_type == 'log':\n",
    "#                 print(\"log loc =\", obj_loc)\n",
    "#                 print(\"agent loc =\", current_state)\n",
    "                if obj_z >= current_state[2] - 0.3 and obj_z <= current_state[2] + 0.3:\n",
    "#                     print(\"log loc =\", obj_loc)\n",
    "#                     print(\"agent loc =\", current_state)\n",
    "                    if obj_x >= current_state[0] - 0.1 and obj_x <= current_state[0] + 0.1:\n",
    "#                         print(\"log loc =\", obj_loc)\n",
    "#                         print(\"agent loc =\", current_state)\n",
    "                        #reward += reward_map['log']\n",
    "#                         print('we hit a log lol z and x: ', reward)\n",
    "                        self.obstacles_hit = True\n",
    "                        time.sleep(0.2) #adding this to pause the overdoing of collision reward dropping.\n",
    "                '''\n",
    "                The below condition seems problematic but the agent seems to be performing\n",
    "                very well with it for some reason. I have tried both current_state[1] -1 and current_state[1].\n",
    "                But it seems like the agent is most likely to hit the wall faster with current_state[1].\n",
    "                '''\n",
    "                if obj_y == current_state[1]:\n",
    "                # if obj_y == current_state[1] - 1:\n",
    "\n",
    "                    #reward += reward_map['log']\n",
    "                    #print('we hit a log lol y: ', reward)\n",
    "                    self.obstacles_hit = True\n",
    "\n",
    "            # Check for diamonds\n",
    "            elif obj_type == 'diamond_block' and obj_z >= current_state[2] and obj_z <= current_state[2] + 1:\n",
    "                #reward = reward_map['diamond_block'] #I feel like this should just be reward = \n",
    "                self.diamond_reached = True\n",
    "#                 print('we hit diamond!!')\n",
    "                time.sleep(0.2)\n",
    "            # Check for water\n",
    "            elif obj_type == 'water' and obj_z >= current_state[2] and obj_z <= current_state[2] + 1:\n",
    "                if obj_x >= current_state[0] - 1.5 and obj_x <= current_state[0] + 1.5:\n",
    "                    reward += reward_map['water']\n",
    "#                     print('puddle lol: ', self.reward)\n",
    "        \n",
    "        if self.obstacles_hit:\n",
    "            reward += reward_map['log']\n",
    "        if self.diamond_reached:\n",
    "            #Instead of commenting out reward, in the case of a diamond getting hit we will just reset it.\n",
    "            reward = self.calculate_dist_reward(agent_host, current_state, timer)\n",
    "            reward += reward_map['diamond_block']\n",
    "        return reward, self.diamond_reached, self.obstacles_hit\n",
    "    \n",
    "    def act(self, agent_host, action, timer): \n",
    "        #lowkey set up an action queue so we dont need\n",
    "        # to hardcode the jumpXmove action. if we have a while loop that just\n",
    "        #runs and constantly time.sleeps() per game state update, we would be able to jump/run\n",
    "        #without more problems. this should be fine for now. The code is just a little messier.\n",
    "#         print(action + \",\", end = \" \")\n",
    "        self.actions_taken.append(action)\n",
    "\n",
    "        term, size = action.split()\n",
    "        if term == 'jump': #pair it with move 1.\n",
    "            agent_host.sendCommand('move 1')\n",
    "            self.actions_taken.append('move 1')\n",
    "        term += ' 0'\n",
    "        \n",
    "        agent_host.sendCommand(action) #GO\n",
    "        time.sleep(0.4)\n",
    "        if term == 'jump 0': #pair it with move 1.\n",
    "            agent_host.sendCommand('move 0')\n",
    "            self.actions_taken.append('move 0')\n",
    "        agent_host.sendCommand(term)  #STOP\n",
    "        \n",
    "        # reevaluate the current state after action\n",
    "#         print('call current location in act')\n",
    "        current_state = self.get_curr_location(agent_host)\n",
    "    \n",
    "        # consider convert locations of curr_state to integers for q_table improvement\n",
    "        curr_state = tuple(map(int, current_state))\n",
    "        print(\"curr_state: \", curr_state)\n",
    "        \n",
    "        if curr_state not in self.q_table:\n",
    "            self.q_table[curr_state] = {}\n",
    "        possible_actions = self.get_possible_actions(agent_host, False)\n",
    "        for action in possible_actions:\n",
    "            if action not in self.q_table[curr_state]:\n",
    "                self.q_table[curr_state][action] = 0                \n",
    "        \n",
    "        # high emphasis on this part since it greatly affects the flow of the entire mission\n",
    "        self.prev_s = curr_state\n",
    "        self.prev_a = action\n",
    "        \n",
    "        current_reward = self.eval_current_state(agent_host, current_state, timer)[0]\n",
    "           \n",
    "        return current_reward, self.diamond_reached, self.obstacles_hit\n",
    "    \n",
    "    def update_q_table_without_tau(self, S, A, R):\n",
    "        \"\"\"Performs relevant updates for the Q-values.\n",
    "        TD(0) implementation of SARSA algorithm. Better for this use case.\n",
    "        Args\n",
    "            S:   <deque>   states queue\n",
    "            A:   <deque>   actions queue\n",
    "            R:   <deque>   rewards queue\n",
    "            T:   <int>      terminating state index\n",
    "\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "#         print(\"R =\", R)\n",
    "#         print(\"S =\", S)\n",
    "#         print(\"A =\", A)\n",
    "        for i in range(len(S)-2, -1, -1):\n",
    "#             print(\"i =\", i)\n",
    "            G = self.gamma * G + R[i + 1]\n",
    "            old_q = self.q_table[S[i]][A[i]]\n",
    "            self.q_table[S[i]][A[i]] = old_q + self.alpha * (G - old_q)\n",
    "            \n",
    "    def updateQTableFromTerminatingState(self, reward):\n",
    "        \"\"\"Change q_table to reflect what we have learnt, after reaching a terminal state.\"\"\"\n",
    "        \n",
    "        # retrieve the old action value from the Q-table (indexed by the previous state and the previous action)\n",
    "        old_q = self.q_table[self.prev_s][self.prev_a]\n",
    "        \n",
    "        # TODO: what should the new action value be?\n",
    "        new_q = old_q\n",
    "        \n",
    "        # assign the new action value to the Q-table\n",
    "        self.q_table[self.prev_s][self.prev_a] = new_q\n",
    "        \n",
    "        \n",
    "    def run(self, agent_host, start_time): \n",
    "        \"\"\"Learns the process to reach the diamonds\"\"\"\n",
    "        S, A, R = deque(), deque(), deque()\n",
    "        total_reward = 0\n",
    "        present_reward = 0\n",
    "        done_update = False\n",
    "        self.prev_s = None\n",
    "        self.prev_a = None\n",
    "        \n",
    "        is_first_action = True\n",
    "    \n",
    "        world_state = agent_host.getWorldState()\n",
    "        while world_state.is_mission_running:\n",
    "            T = sys.maxsize\n",
    "            present_reward = 0\n",
    "            \n",
    "            if is_first_action:\n",
    "                # wait until we have received a valid observation\n",
    "                while True:\n",
    "                    time.sleep(0.1)\n",
    "                    world_state = agent_host.getWorldState()\n",
    "                    \n",
    "                    # scope for any errors that may occur in the world state\n",
    "                    for error in world_state.errors:\n",
    "                        self.logger.error(\"Error: %s\" % error.text)\n",
    "                        \n",
    "                    # gather rewards for any found in world state\n",
    "                    for reward in world_state.rewards:\n",
    "                        present_reward += reward.getValue()\n",
    "                         \n",
    "                    # where the actual actions take place\n",
    "                    if world_state.is_mission_running and len(world_state.observations)>0 and not world_state.observations[-1].text==\"{}\":\n",
    "                        term_flag = self.diamond_reached or self.obstacles_hit\n",
    "                        \n",
    "                        possible_actions = self.get_possible_actions(agent_host, term_flag)\n",
    "                        \n",
    "                        s0 = self.get_curr_location(agent_host)\n",
    "                        s0 = tuple(map(int, s0))\n",
    "                        \n",
    "                        a0 = self.choose_action(s0, possible_actions, self.epsilon)\n",
    "                        \n",
    "                        S.append(s0)\n",
    "                        A.append(a0)\n",
    "                        R.append(0) # should we append returned_reward for a0 after self.act??\n",
    "                        \n",
    "#                         print(\"taking action: \", a0)\n",
    "                        # agent_host.sendCommand(a0);\n",
    "                        \n",
    "                        returned_reward, diamond_reached, obstacles_hit = self.act(agent_host, a0,start_time)\n",
    "                        time.sleep(0.2)\n",
    "#                         print(\"returned_reward: \", returned_reward)\n",
    "#                         print(\"diamond_reached: \", diamond_reached)\n",
    "#                         print(\"obstacles_hit?? \", obstacles_hit)\n",
    "#                         print(\"self.obstacles_hit =\", self.obstacles_hit)\n",
    "                        R.append(returned_reward)\n",
    "                        \n",
    "                        term_flag = self.diamond_reached or self.obstacles_hit\n",
    "#                         print(\"term flag =\", term_flag)\n",
    "                        \n",
    "                        total_reward += returned_reward\n",
    "                        # update Q values\n",
    "#                         print(\"S =\", S)\n",
    "#                         print(\"A =\", A)\n",
    "#                         print(\"R =\", R)\n",
    "                        if not term_flag and self.prev_s is not None and self.prev_a is not None:\n",
    "                            self.update_q_table_without_tau(S, A, R)\n",
    "                        \n",
    "                        # Terminating state\n",
    "                        # no need to go any further if we hit certain obstacle or goal\n",
    "                        # not using tau values for this implementation\n",
    "                        if term_flag:\n",
    "                            # calculating number of obstacles avoided\n",
    "                            curr_state = self.get_curr_location(agent_host)\n",
    "                            obstacles = logs + water\n",
    "                            obs_count = 0\n",
    "\n",
    "                            for obs in obstacles:\n",
    "                                obs_coords = obs[1]\n",
    "                                if obs_coords[2] < curr_state[2]:\n",
    "                                    obs_count += 1\n",
    "                            self.obstacles_avoided = obs_count\n",
    "\n",
    "                            # we should be calculating reward if it hits a log.\n",
    "                            present_reward = returned_reward  # either = or +=\n",
    "#                             print(\"Reward:\", present_reward)\n",
    "                            \n",
    "#                             while len(S) > 1:\n",
    "#                                 self.update_q_table_without_tau(S, A, R)\n",
    "                            \n",
    "                            # process final reward\n",
    "                            self.logger.debug(\"Final reward: %d\" % present_reward)\n",
    "                            total_reward += present_reward\n",
    "\n",
    "                            # update Q values\n",
    "                            if self.prev_s is not None and self.prev_a is not None:\n",
    "#                                 self.updateQTableFromTerminatingState( present_reward )\n",
    "                                self.update_q_table_without_tau(S, A, R)\n",
    "\n",
    "                            return total_reward       \n",
    "                        else:\n",
    "                            s = self.get_curr_location(agent_host)\n",
    "                            s = tuple(map(int, s))\n",
    "                \n",
    "                            S.append(s)\n",
    "                            possible_actions = self.get_possible_actions(agent_host, term_flag)\n",
    "                            next_a = self.choose_action(s, possible_actions, self.epsilon)\n",
    "                            A.append(next_a)\n",
    "                        \n",
    "                        break\n",
    "                    \n",
    "                    if not world_state.is_mission_running:\n",
    "                        break\n",
    "#                 print(\"first!!\")\n",
    "                is_first_action = False\n",
    "            else:\n",
    "                # wait for non-zero reward\n",
    "                ##############################################################\n",
    "#                 while world_state.is_mission_running and present_reward == 0:\n",
    "#                     time.sleep(0.1)\n",
    "#                     print(\"NO CAPPP\")\n",
    "#                     world_state = agent_host.getWorldState()\n",
    "                    \n",
    "#                     # scope for any errors that may occur in the world state\n",
    "#                     for error in world_state.errors:\n",
    "#                         self.logger.error(\"Error: %s\" % error.text)\n",
    "                        \n",
    "#                     # gather rewards for any found in world state\n",
    "#                     for reward in world_state.rewards:\n",
    "#                         present_reward += reward.getValue()\n",
    "                ##############################################################\n",
    "\n",
    "                # allow time to stabilise after action\n",
    "                while True:\n",
    "                    time.sleep(0.1)\n",
    "                    world_state = agent_host.getWorldState()\n",
    "                    \n",
    "                    # scope for any errors that may occur in the world state\n",
    "                    for error in world_state.errors:\n",
    "                        self.logger.error(\"Error: %s\" % error.text)\n",
    "                        \n",
    "                    # gather rewards for any found in world state\n",
    "                    for reward in world_state.rewards:\n",
    "                        present_reward += reward.getValue()\n",
    "                    \n",
    "                    # where the actual actions take place\n",
    "                    if world_state.is_mission_running and len(world_state.observations)>0 and not world_state.observations[-1].text==\"{}\":\n",
    "                        term_flag = self.diamond_reached or self.obstacles_hit\n",
    "                        \n",
    "                        s = self.get_curr_location(agent_host)\n",
    "                        s = tuple(map(int, s))\n",
    "                        \n",
    "                        S.append(s)\n",
    "                        possible_actions = self.get_possible_actions(agent_host, term_flag)\n",
    "                        next_a = self.choose_action(s, possible_actions, self.epsilon)\n",
    "                        A.append(next_a)\n",
    "                        \n",
    "#                         print(\"taking action: \", a0)\n",
    "                        # agent_host.sendCommand(a0);\n",
    "                        \n",
    "                        returned_reward, diamond_reached, obstacles_hit = self.act(agent_host, a0,start_time)\n",
    "                        time.sleep(0.2)\n",
    "#                         print(\"returned_reward: \", returned_reward)\n",
    "#                         print(\"diamond_reached: \", diamond_reached)\n",
    "#                         print(\"obstacles_hit?? \", obstacles_hit)\n",
    "                        \n",
    "                        term_flag = self.diamond_reached or self.obstacles_hit\n",
    "#                         print(\"term flag =\", term_flag)\n",
    "                        \n",
    "                        total_reward += returned_reward\n",
    "                        R.append(returned_reward)\n",
    "                        # update Q values\n",
    "                        if not term_flag and self.prev_s is not None and self.prev_a is not None:\n",
    "                            self.update_q_table_without_tau(S, A, R)\n",
    "                        \n",
    "                        # Terminating state\n",
    "                        # no need to go any further if we hit certain obstacle or goal\n",
    "                        # not using tau values for this implementation\n",
    "                        if term_flag:\n",
    "                            # calculating number of obstacles avoided\n",
    "                            curr_state = self.get_curr_location(agent_host)\n",
    "                            obstacles = logs + water\n",
    "                            obs_count = 0\n",
    "\n",
    "                            for obs in obstacles:\n",
    "                                obs_coords = obs[1]\n",
    "                                if obs_coords[2] < curr_state[2]:\n",
    "                                    obs_count += 1\n",
    "                            self.obstacles_avoided = obs_count\n",
    "                            \n",
    "                            # we should be calculating reward if it hits a log.\n",
    "                            present_reward = returned_reward  # either = or +=\n",
    "#                             print(\"Reward:\", present_reward)\n",
    "                            \n",
    "#                             while len(S) > 1:\n",
    "#                                 self.update_q_table_without_tau(S, A, R)\n",
    "                            \n",
    "                            # process final reward\n",
    "                            self.logger.debug(\"Final reward: %d\" % present_reward)\n",
    "                            total_reward += present_reward\n",
    "\n",
    "                            # update Q values\n",
    "                            if self.prev_s is not None and self.prev_a is not None:\n",
    "#                                 self.updateQTableFromTerminatingState( present_reward \n",
    "                                self.update_q_table_without_tau(S, A, R)\n",
    "                            return total_reward       \n",
    "                        \n",
    "                        break\n",
    "                    \n",
    "                    if not world_state.is_mission_running:\n",
    "                        break\n",
    "    \n",
    "        # process final reward\n",
    "        self.logger.debug(\"Final reward: %d\" % present_reward)\n",
    "        total_reward += present_reward\n",
    "\n",
    "        # update Q values\n",
    "        if self.prev_s is not None and self.prev_a is not None:\n",
    "            self.updateQTableFromTerminatingState( present_reward )\n",
    "            \n",
    "#         print(\"diamond_reached: \", diamond_reached)\n",
    "                \n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)  # flush print output immediately\n",
    "    print('Starting...', flush=True)\n",
    "    q_table_save_frequency = 1  # save the Q-table after every 100 iterations\n",
    "    q_table_file = \"q_table.pckl\"\n",
    "    \n",
    "    my_client_pool = MalmoPython.ClientPool()\n",
    "    my_client_pool.add(MalmoPython.ClientInfo(\"127.0.0.1\", 10000))\n",
    "\n",
    "    agent_host = MalmoPython.AgentHost()\n",
    "    try:\n",
    "        agent_host.parse(sys.argv)\n",
    "    except RuntimeError as e:\n",
    "        print('ERROR:', e)\n",
    "        print(agent_host.getUsage())\n",
    "        exit(1)\n",
    "    if agent_host.receivedArgument(\"help\"):\n",
    "        print(agent_host.getUsage())\n",
    "        exit(0)\n",
    "    \n",
    "    print(world_num)  # defined above when finding obstacle coords\n",
    "    mission_file = 'xmls/world_{world_num}.txt'.format(world_num = world_num)\n",
    "\n",
    "    with open(mission_file, 'r') as f:\n",
    "        print(\"Loading mission from %s\" % mission_file)\n",
    "        missionXML = f.read()\n",
    "\n",
    "    \n",
    "    num_reps = 250\n",
    "    run_times = []\n",
    "    finished_track = []\n",
    "    actions_taken = []\n",
    "    obstacles_avoided = []\n",
    "        \n",
    "    '''\n",
    "    alpha:  <float>  learning rate      (default = 0.3)\n",
    "    gamma:  <float>  value decay rate   (default = 1)\n",
    "    epsilon <int>    chance of taking a random action instead of the best   (default = 0.3)\n",
    "    '''\n",
    "    \n",
    "    # test with epsilon range\n",
    "    epsilon = 0.3  # test with range 0.3 to 1\n",
    "    alpha = 0.3    # test with range 0.3 to 1\n",
    "    gamma = 1      # test with range 1 to 3\n",
    "    racer = Racer(alpha, gamma, epsilon)\n",
    "    \n",
    "    print(\"alpha=\", racer.alpha)\n",
    "    print(\"gamma=\", racer.gamma)        \n",
    "    print(\"epsilon=\", racer.epsilon)\n",
    "        \n",
    "    racer.clear_actions()\n",
    "    \n",
    "    \n",
    "    cumulative_rewards = []\n",
    "    for iRepeat in range(num_reps):\n",
    "        my_mission = MalmoPython.MissionSpec(missionXML, True)\n",
    "        my_mission_record = MalmoPython.MissionRecordSpec()  # Records nothing by default\n",
    "        my_mission.requestVideo(1260, 960)\n",
    "        my_mission.setViewpoint(0)\n",
    "        \n",
    "        # attempt to start a mission\n",
    "        max_retries = 3\n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                agent_host.startMission( my_mission, my_client_pool, my_mission_record, 0, \"Racer\")\n",
    "                break\n",
    "            except RuntimeError as e:\n",
    "                if retry == max_retries - 1:\n",
    "                    print(\"Error starting mission\", e)\n",
    "                    print(\"Is the game running?\")\n",
    "                    exit(1)\n",
    "                else:\n",
    "                    time.sleep(2)\n",
    "\n",
    "        # Loop until mission starts:\n",
    "        print(\"\\nWaiting for the mission to start on trial\", iRepeat+1)\n",
    "        world_state = agent_host.getWorldState()\n",
    "        \n",
    "        while not world_state.has_mission_begun:\n",
    "            time.sleep(0.1)\n",
    "            world_state = agent_host.getWorldState()\n",
    "            \n",
    "            for error in world_state.errors:\n",
    "                print(\"Error:\",error.text) \n",
    "                \n",
    "        print(\"Mission running...\")\n",
    "        \n",
    "        # log time for each run\n",
    "        start_time = time.time()\n",
    "        \n",
    "        cumulative_reward = racer.run(agent_host, start_time)\n",
    "        print('Cumulative reward: %d' % cumulative_reward)\n",
    "        cumulative_rewards += [ cumulative_reward ]\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(\"--- %s seconds ---\" % (time_elapsed))\n",
    "        run_times.append(time_elapsed)\n",
    "        \n",
    "        print(\"diamond reached? \", racer.diamond_reached)\n",
    "        finished_track.append(racer.diamond_reached)\n",
    "        \n",
    "        print(\"Number of obstacles avoided: \", racer.obstacles_avoided)\n",
    "        obstacles_avoided.append(racer.obstacles_avoided)\n",
    "\n",
    "        \n",
    "#         with open('q_table.pckl', 'wb') as f:\n",
    "#             pickle.dump(racer.q_table, f)\n",
    "#         print('Q-table saved.')\n",
    "#         with open('q_table.pkl', 'rb') as read_q:\n",
    "#             q_table = pickle.load(read_q)\n",
    "#             print(q_table)\n",
    "        \n",
    "        actions_taken.append(racer.actions_taken)\n",
    "    \n",
    "        racer.clear_actions()\n",
    "        time.sleep(1)\n",
    "        \n",
    "\n",
    "print(\"\\n\\nMission ended.\")\n",
    "print(\"Cumulative rewards for all %d runs:\" % num_reps)\n",
    "print(cumulative_rewards)\n",
    "\n",
    "print(\"\\nRun times for all %d runs in seconds:\" % num_reps)\n",
    "print(run_times)\n",
    "\n",
    "for i in range(0, num_reps):\n",
    "    print(\"\\nAgent had overall score of %s on trial %s with a run time of %.4f seconds. finished track? %s \" % (cumulative_rewards[i], i+1, run_times[i], finished_track[i]))\n",
    "    \n",
    "print(\"\\n\\nq_table: \", racer.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe table \n",
    "# iteration number\n",
    "# number of obstacles avoided\n",
    "# reward\n",
    "# time\n",
    "# number of actions taken\n",
    "print(\"Creating dataframe...\")\n",
    "print(\"epsilon=\", racer.epsilon)\n",
    "print(\"alpha=\", racer.alpha)\n",
    "print(\"gamma=\", racer.gamma) \n",
    "\n",
    "num_actions_taken = []\n",
    "for i in range(len(actions_taken)):\n",
    "    num_actions_taken.append(len(actions_taken[i]))\n",
    "    \n",
    "df = pd.DataFrame(list(zip(cumulative_rewards, run_times, obstacles_avoided, finished_track, num_actions_taken)), \n",
    "                  columns=['Reward', 'Run time(s)', 'Obstacles avoided', 'Finished track?', 'Number of actions taken'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting time elapsed per run (sec)...\")\n",
    "print(\"epsilon=\", racer.epsilon)\n",
    "print(\"alpha=\", racer.alpha)\n",
    "print(\"gamma=\", racer.gamma) \n",
    "\n",
    "        \n",
    "max_run_time = max(run_times)\n",
    "max_run_time_index = run_times.index(max_run_time)\n",
    "\n",
    "min_run_time = min(run_times)\n",
    "min_run_time_index = run_times.index(min_run_time)\n",
    "\n",
    "x = []\n",
    "for i in range(num_reps):\n",
    "    x.append(i)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 8));\n",
    "plt.xlim(-1, num_reps+1);\n",
    "plt.title(\"Time elapsed per run (sec)\");\n",
    "plt.plot(x, run_times, label=\"Time (sec)\");\n",
    "\n",
    "# plot if agent finished track\n",
    "finished_track_x = [i for i, x in enumerate(finished_track) if x]\n",
    "if finished_track_x:\n",
    "    finished_track_y = []\n",
    "    for i in range(max(finished_track_x)+1):\n",
    "        if i in finished_track_x:\n",
    "            finished_track_y.append(run_times[i])\n",
    "    \n",
    "    plt.plot(finished_track_x, finished_track_y, 'rx', label=\"Finished track\");\n",
    "\n",
    "plt.axvline(max_run_time_index, color='k', linestyle='--', linewidth=3, label=\"Maximum = {max_run_time}s\".format(max_run_time=round(max_run_time, 2)))\n",
    "plt.axvline(min_run_time_index, color='k', linestyle=':', linewidth=3, label=\"Minimum = {min_run_time}s\".format(min_run_time=round(min_run_time, 2)))\n",
    "        \n",
    "plt.plot([], [], ' ', label=\"epsilon = {epsilon}\".format(epsilon=racer.epsilon))\n",
    "plt.plot([], [], ' ', label=\"alpha = {alpha}\".format(alpha=racer.alpha))\n",
    "plt.plot([], [], ' ', label=\"gamma = {gamma}\".format(gamma=racer.gamma))\n",
    "    \n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('time_per_run.jpg');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting total reward per run...\")\n",
    "print(\"epsilon=\", racer.epsilon)\n",
    "print(\"alpha=\", racer.alpha)\n",
    "print(\"gamma=\", racer.gamma)  \n",
    "\n",
    "        \n",
    "max_reward = max(cumulative_rewards)\n",
    "max_reward_index = cumulative_rewards.index(max_reward)\n",
    "\n",
    "min_reward = min(cumulative_rewards)\n",
    "min_reward_index = cumulative_rewards.index(min_reward)\n",
    "\n",
    "x = []\n",
    "for i in range(num_reps):\n",
    "    x.append(i)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 8));\n",
    "plt.xlim(-1, num_reps+1);\n",
    "plt.title(\"Total reward per run\")\n",
    "plt.plot(x, cumulative_rewards, 'y-', label=\"Total reward\")\n",
    "\n",
    "# plot if agent finished track\n",
    "finished_track_x = [i for i, x in enumerate(finished_track) if x]\n",
    "if finished_track_x:\n",
    "    finished_track_y = []\n",
    "    for i in range(max(finished_track_x)+1):\n",
    "        if i in finished_track_x:\n",
    "            finished_track_y.append(cumulative_rewards[i])\n",
    "    plt.plot(finished_track_x, finished_track_y, 'rx', label=\"Finished track\");\n",
    "\n",
    "plt.axvline(max_reward_index, color='k', linestyle='--', linewidth=3, label=\"Maximum = {max_reward}\".format(max_reward=int(max_reward)))\n",
    "plt.axvline(min_reward_index, color='k', linestyle=':', linewidth=3, label=\"Minimum = {min_reward}\".format(min_reward=int(min_reward)))\n",
    "\n",
    "plt.plot([], [], ' ', label=\"epsilon = {epsilon}\".format(epsilon=racer.epsilon))\n",
    "plt.plot([], [], ' ', label=\"alpha = {alpha}\".format(alpha=racer.alpha))\n",
    "plt.plot([], [], ' ', label=\"gamma = {gamma}\".format(gamma=racer.gamma))\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('reward_per_run.jpg');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting number of actions taken per run...\")\n",
    "print(\"epsilon=\", racer.epsilon)\n",
    "print(\"alpha=\", racer.alpha)\n",
    "print(\"gamma=\", racer.gamma)  \n",
    "\n",
    "\n",
    "x = []\n",
    "for i in range(num_reps):\n",
    "    x.append(i)\n",
    "    \n",
    "y = []\n",
    "for i in range(len(actions_taken)):\n",
    "    y.append(len(actions_taken[i]))\n",
    "    \n",
    "max_actions_taken = max(y)\n",
    "max_actions_taken_index = y.index(max_actions_taken)\n",
    "\n",
    "min_actions_taken = min(y)\n",
    "min_actions_taken_index = y.index(min_actions_taken)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.xlim(-1, num_reps+1);\n",
    "plt.title(\"Number of actions taken per run\")\n",
    "plt.plot(x, y, 'g-', label=\"Number of actions taken\")\n",
    "\n",
    "# plot if agent finished track\n",
    "finished_track_x = [i for i, x in enumerate(finished_track) if x]\n",
    "if finished_track_x:\n",
    "    finished_track_y = []\n",
    "    for i in range(max(finished_track_x)+1):\n",
    "        if i in finished_track_x:\n",
    "            finished_track_y.append(len(actions_taken[i]))\n",
    "\n",
    "    plt.plot(finished_track_x, finished_track_y, 'rx', label=\"Finished track\");\n",
    "\n",
    "plt.axvline(max_actions_taken_index, color='k', linestyle='--', linewidth=3, label=\"Maximum = {max_actions_taken}\".format(max_actions_taken=int(max_actions_taken)))\n",
    "plt.axvline(min_actions_taken_index, color='k', linestyle=':', linewidth=3, label=\"Minimum = {min_actions_taken}\".format(min_actions_taken=int(min_actions_taken)))\n",
    "\n",
    "plt.plot([], [], ' ', label=\"epsilon = {epsilon}\".format(epsilon=racer.epsilon))\n",
    "plt.plot([], [], ' ', label=\"alpha = {alpha}\".format(alpha=racer.alpha))\n",
    "plt.plot([], [], ' ', label=\"gamma = {gamma}\".format(gamma=racer.gamma))\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('actions_taken_per_run.jpg');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting total number of obstacles avoided per run...\")\n",
    "print(\"epsilon=\", racer.epsilon)\n",
    "print(\"alpha=\", racer.alpha)\n",
    "print(\"gamma=\", racer.gamma)  \n",
    "\n",
    "        \n",
    "max_obs = max(obstacles_avoided)\n",
    "max_obs_index = obstacles_avoided.index(max_obs)\n",
    "\n",
    "min_obs = min(obstacles_avoided)\n",
    "min_obs_index = obstacles_avoided.index(min_obs)\n",
    "\n",
    "x = []\n",
    "for i in range(num_reps):\n",
    "    x.append(i)\n",
    "\n",
    "plt.figure(figsize=(16, 8));\n",
    "plt.xlim(-1, num_reps+1);\n",
    "plt.title(\"Total obstacles avoided per run\")\n",
    "plt.plot(x, obstacles_avoided, 'y-', label=\"Total reward\")\n",
    "\n",
    "# plot if agent finished track\n",
    "finished_track_x = [i for i, x in enumerate(finished_track) if x]\n",
    "if finished_track_x:\n",
    "    finished_track_y = []\n",
    "    for i in range(max(finished_track_x)+1):\n",
    "        if i in finished_track_x:\n",
    "            finished_track_y.append(obstacles_avoided[i])\n",
    "    plt.plot(finished_track_x, finished_track_y, 'rx', label=\"Finished track\");\n",
    "    \n",
    "plt.axvline(max_obs_index, color='k', linestyle='--', linewidth=3, label=\"Maximum = {max_obs}\".format(max_obs=int(max_obs)))\n",
    "plt.axvline(min_obs_index, color='k', linestyle=':', linewidth=3, label=\"Minimum = {min_obs}\".format(min_obs=int(min_obs)))\n",
    "\n",
    "plt.plot([], [], ' ', label=\"epsilon = {epsilon}\".format(epsilon=racer.epsilon))\n",
    "plt.plot([], [], ' ', label=\"alpha = {alpha}\".format(alpha=racer.alpha))\n",
    "plt.plot([], [], ' ', label=\"gamma = {gamma}\".format(gamma=racer.gamma))\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('obstacles_avoided_per_run.jpg');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "q_table_data = []\n",
    "for state in racer.q_table:\n",
    "#     print(state, racer.q_table[state])\n",
    "    q_table_data.append([state, racer.q_table[state]])\n",
    "\n",
    "print(tabulate(q_table_data, headers=[\"State\", \"Actions\"]))\n",
    "\n",
    "with open('q_table.txt', 'w') as f:\n",
    "    f.write(tabulate(q_table_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
